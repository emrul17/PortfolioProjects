{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Price Prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ask a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n",
    "\n",
    "With 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, our goal is to predict the price of each house."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Data Analysis Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary python libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading  data\n",
    "def load_data(df):\n",
    "    return pd.read_csv(df)\n",
    "\n",
    "#Storing Id\n",
    "def store_id(df):\n",
    "    return df['Id']\n",
    "\n",
    "# Dropping Id\n",
    "def drop_id(df):\n",
    "    return df.drop(\"Id\", axis = 1, inplace = True)\n",
    "\n",
    "# merging the data \n",
    "def full_data(df1,df2):\n",
    "    return pd.concat(objs=[df1, df2], axis=0).reset_index(drop=True)\n",
    "\n",
    "# Imputing missing value\n",
    "def miss_val(df):\n",
    "    for col in miss_col1:\n",
    "        if df[col].dtype=='O':\n",
    "            df[col]=df[col].fillna(\"None\")\n",
    "        else:\n",
    "            df[col]=df[col].fillna(0)\n",
    "    \n",
    "    df['LotFrontage']=df.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))\n",
    "    df['Electrical']=df['Electrical'].fillna(df['Electrical'].mode()[0]) \n",
    "    \n",
    "    return df\n",
    "\n",
    "# creating features\n",
    "def new_features(df):\n",
    "    df['TotSF']=df['TotalBsmtSF']+df['1stFlrSF']+df['2ndFlrSF']\n",
    "    df['TotArea']=df['GarageArea']+df['GrLivArea']\n",
    "    \n",
    "    return df\n",
    "\n",
    "#numerical to categorical col, some columns are numerical but actually they are categorical\n",
    "def num_to_cat(df):\n",
    "    for col in cols:\n",
    "        df[col] = df[col].apply(str)\n",
    "    return df\n",
    "\n",
    "\n",
    "# label Encoding for ordinal data\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encode=LabelEncoder()\n",
    "\n",
    "def convert_data(df):\n",
    "    for col in ordinal_cat:\n",
    "        df[col]=label_encode.fit_transform(df[col])\n",
    "    return pd.get_dummies(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Test data\n",
    "df_train=load_data('train.csv')\n",
    "df_test=load_data('test.csv')\n",
    "\n",
    "# store id\n",
    "id_train= df_train['Id']\n",
    "Id_test = df_test['Id']\n",
    "\n",
    "# Droping Id\n",
    "drop_id(df_train)\n",
    "drop_id(df_test)\n",
    "\n",
    "# remove suspicious point\n",
    "df_train=df_train[df_train['SalePrice']<700000]\n",
    "\n",
    "# Log transformation\n",
    "df_train[\"SalePrice\"] = np.log1p(df_train[\"SalePrice\"])\n",
    "\n",
    "# Removing suspicious outliers\n",
    "df_train = df_train.drop(df_train[(df_train['GrLivArea']>4000) & (df_train['SalePrice']<300000)].index).reset_index(drop=True)\n",
    "df_train=df_train.drop(df_train[(df_train['LotFrontage']>250) & (df_train['SalePrice']<300000)].index).reset_index(drop=True)\n",
    "df_train=df_train.drop(df_train[(df_train['BsmtFinSF1']>1400) & (df_train['SalePrice']<400000)].index).reset_index(drop=True)\n",
    "df_train=df_train.drop(df_train[(df_train['TotalBsmtSF']>5000) & (df_train['SalePrice']<300000)].index).reset_index(drop=True)\n",
    "df_train=df_train.drop(df_train[(df_train['1stFlrSF']>4000) & (df_train['SalePrice']<300000)].index).reset_index(drop=True)\n",
    "\n",
    "# Length of the training data\n",
    "train_len = len(df_train)\n",
    "\n",
    "# Merging train and test data\n",
    "data=full_data(df_train, df_test)\n",
    "\n",
    "\n",
    "# Filling missing value\n",
    "miss_col1=['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'GarageCond', 'GarageType', 'GarageYrBlt',\n",
    "           'GarageFinish', 'GarageQual', 'BsmtExposure','BsmtFinType2', 'BsmtFinType1', 'BsmtCond', 'BsmtQual', \n",
    "           'MasVnrArea', 'MasVnrType','SaleType','MSZoning','Utilities','Functional','Exterior1st','Exterior2nd',\n",
    "           'BsmtFinSF1','BsmtFinSF2','TotalBsmtSF','GarageArea','KitchenQual','GarageCars','BsmtFullBath',\n",
    "           'BsmtHalfBath','BsmtUnfSF']\n",
    "\n",
    "# Remving missing value\n",
    "clean_data=miss_val(data)\n",
    "\n",
    "# Creating new features\n",
    "clean_data=new_features(clean_data)\n",
    "\n",
    "# Columns that are numerical but actually they are categorical\n",
    "cols=['MSSubClass','OverallCond','YrSold','MoSold']\n",
    "clean_data=num_to_cat(clean_data)\n",
    "\n",
    "\n",
    "# Ordinal features\n",
    "ordinal_cat=['OverallCond','KitchenQual','YrSold','MoSold','Fence','PoolQC','FireplaceQu','GarageQual', \n",
    "             'GarageCond','LotShape','LandSlope','HouseStyle','ExterQual','ExterCond','BsmtQual', \n",
    "             'BsmtCond','BsmtExposure','BsmtFinType1', 'BsmtFinType2','HeatingQC','KitchenQual','CentralAir',\n",
    "             'MSSubClass']\n",
    "\n",
    "# converting categorical to numerical features\n",
    "clean_data=convert_data(clean_data)\n",
    "\n",
    "# Preparing data\n",
    "df_target=clean_data['SalePrice']\n",
    "df_feature=clean_data.drop(columns=['SalePrice'])\n",
    "X_train=df_feature[:train_len]\n",
    "Y_train=df_target[:train_len]\n",
    "X_test=df_feature[train_len:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Machine Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import MLlibraries\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.linear_model import ElasticNet, Lasso, Ridge, BayesianRidge, LassoLarsIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic models\n",
    "Basic_model=[\n",
    "    Lasso(),\n",
    "    RandomForestRegressor(),\n",
    "    ElasticNet(),\n",
    "    KernelRidge(),\n",
    "    GradientBoostingRegressor()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Root Mean Square Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result:\n",
      "    index                      Model      RMSE\n",
      "0      3                KernelRidge  0.120578\n",
      "1      4  GradientBoostingRegressor  0.122217\n",
      "2      1      RandomForestRegressor  0.143142\n",
      "3      2                 ElasticNet  0.161444\n",
      "4      0                      Lasso  0.169176\n",
      "The minimum RMSE is 0.12057848837200343 and the best model is KernelRidge\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate root mean squared error with default parameters\n",
    "n_folds = 5 # five fold cross validation\n",
    "def rmse_cv(models):\n",
    "    \n",
    "    model_name=[]\n",
    "    RMSE=[]\n",
    "    for model in models:\n",
    "        kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(X_train)\n",
    "        rmse= np.sqrt(-cross_val_score(model, X_train, Y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "        mean_RMSE=np.mean(rmse)\n",
    "        \n",
    "        model_name.append(model.__class__.__name__)\n",
    "        RMSE.append(mean_RMSE)\n",
    "    \n",
    "    result=pd.DataFrame({'Model':model_name,'RMSE':RMSE})\n",
    "    \n",
    "    result=result.sort_values(by='RMSE',ascending=True).reset_index()\n",
    "    print(\"\\nResult:\\n\", result)\n",
    "    \n",
    "    min_RMSE=min(result['RMSE'])\n",
    "    best_model=result['Model'][0]# first row and first column\n",
    "    \n",
    "    print('The minimum RMSE is {} and the best model is {}'.format(min_RMSE,best_model))\n",
    "        \n",
    "rmse_cv(Basic_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print best estimator for lasso: Lasso(alpha=0.0005, copy_X=True, fit_intercept=True, max_iter=1000,\n",
      "   normalize=False, positive=False, precompute=False, random_state=42,\n",
      "   selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Print best estimator for RandomForest: RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=50, min_samples_split=2,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=150, n_jobs=1,\n",
      "           oob_score=False, random_state=42, verbose=0, warm_start=False)\n",
      "Print best estimator for ElasticNet: ElasticNet(alpha=0.0006, copy_X=True, fit_intercept=True, l1_ratio=0.5,\n",
      "      max_iter=1000, normalize=False, positive=False, precompute=False,\n",
      "      random_state=42, selection='cyclic', tol=0.0001, warm_start=False)\n",
      "Print best estimator for KernelRidge: KernelRidge(alpha=0.0004, coef0=1, degree=3, gamma=None, kernel='linear',\n",
      "      kernel_params=None)\n",
      "Print best estimator for GradientBoostingRegressor: GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
      "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "             min_impurity_split=None, min_samples_leaf=1,\n",
      "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "             n_estimators=4000, presort='auto', random_state=None,\n",
      "             subsample=1.0, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "# Grid search Cv method to find the best parameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "Lasso_params = {'alpha': [0.0001,0.0002,0.0003,0.0004,0.0005,0.0006]}\n",
    "RandomForest_params = {'n_estimators': list(range(50, 200, 25)), 'max_features': ['auto', 'sqrt', 'log2'], \n",
    "         'min_samples_leaf': list(range(50, 200, 50))}\n",
    "ElasticNet_params = {'alpha': [0.0001,0.0002,0.0003,0.0004,0.0005,0.0006]}\n",
    "Ridge_params = {'alpha': [0.0001,0.0002,0.0003,0.0004,0.0005,0.0006]}\n",
    "Grad_boosting_params = {'n_estimators': [1000,2000,3000,4000,5000,6000]}\n",
    "\n",
    "# Grid Search for Lasso \n",
    "grid_search_cv = GridSearchCV(Lasso(random_state=42), Lasso_params, n_jobs=-1)\n",
    "grid_search_cv.fit(X_train, Y_train)\n",
    "\n",
    "print('Print best estimator for lasso:', grid_search_cv.best_estimator_)\n",
    "\n",
    "# Grid Search for RandomForest\n",
    "\n",
    "grid_search_cv = GridSearchCV(RandomForestRegressor(random_state=42), RandomForest_params, n_jobs=-1)\n",
    "grid_search_cv.fit(X_train, Y_train)\n",
    "\n",
    "print('Print best estimator for RandomForest:', grid_search_cv.best_estimator_)\n",
    "\n",
    "# Grid Search for ElasticNet\n",
    "grid_search_cv = GridSearchCV(ElasticNet(random_state=42), ElasticNet_params, n_jobs=-1)\n",
    "grid_search_cv.fit(X_train, Y_train)\n",
    "\n",
    "print('Print best estimator for ElasticNet:', grid_search_cv.best_estimator_)\n",
    "\n",
    "# Grid Search for KernelRidge\n",
    "grid_search_cv = GridSearchCV(KernelRidge(), Ridge_params, n_jobs=-1)\n",
    "grid_search_cv.fit(X_train, Y_train)\n",
    "\n",
    "print('Print best estimator for KernelRidge:', grid_search_cv.best_estimator_)\n",
    "\n",
    "# Grid Search for GradientBoosting Regressor\n",
    "grid_search_cv = GridSearchCV(GradientBoostingRegressor(), Grad_boosting_params, n_jobs=-1)\n",
    "grid_search_cv.fit(X_train, Y_train)\n",
    "\n",
    "print('Print best estimator for GradientBoostingRegressor:', grid_search_cv.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSE with best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result:\n",
      "    index                      Model      RMSE\n",
      "0      0                      Lasso  0.113286\n",
      "1      2                 ElasticNet  0.113730\n",
      "2      4  GradientBoostingRegressor  0.115949\n",
      "3      3                KernelRidge  0.129147\n",
      "4      1      RandomForestRegressor  0.174109\n",
      "The best model and RMSE for best parameter are Lasso and 0.11328604728238068 respectively\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate root mean squared error with best parameters\n",
    "model_best_params=[\n",
    "    Lasso(alpha =0.0005, random_state=42),\n",
    "    RandomForestRegressor(min_samples_leaf=50, min_samples_split=2, n_estimators=150, random_state=42),\n",
    "    ElasticNet(alpha=0.0006, l1_ratio=.5, random_state=42),\n",
    "    KernelRidge(alpha=0.004, kernel='linear', degree=3, coef0=1.0),\n",
    "    GradientBoostingRegressor(alpha=0.9, n_estimators=1000, learning_rate=0.1,\n",
    "                                   max_depth=3, max_features='sqrt',\n",
    "                                   min_samples_leaf=1, min_samples_split=2, \n",
    "                                   loss='huber', random_state =42)\n",
    "]\n",
    "\n",
    "def rmse_cv(models):\n",
    "    \n",
    "    model_name=[]\n",
    "    RMSE=[]\n",
    "    for model in models:\n",
    "        kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(X_train)\n",
    "        rmse= np.sqrt(-cross_val_score(model, X_train, Y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "        mean_RMSE=np.mean(rmse)\n",
    "        \n",
    "        model_name.append(model.__class__.__name__)\n",
    "        RMSE.append(mean_RMSE)\n",
    "    \n",
    "    result=pd.DataFrame({'Model':model_name,'RMSE':RMSE})\n",
    "    \n",
    "    result=result.sort_values(by='RMSE',ascending=True).reset_index()\n",
    "    print(\"\\nResult:\\n\", result)\n",
    "    \n",
    "    min_RMSE=min(result['RMSE'])\n",
    "    best_model=result['Model'][0]# first row and first column\n",
    "    \n",
    "    print('The best model and RMSE for best parameter are {} and {} respectively'.format(best_model, min_RMSE))\n",
    "        \n",
    "rmse_cv(model_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction with lasso\n",
    "lasso_fit= Lasso(alpha =0.0005, random_state=42).fit(X_train,Y_train)\n",
    "y_pred=np.expm1(lasso_fit.predict(X_test))\n",
    "test_prediction = pd.Series(y_pred, name=\"SalePrice\")\n",
    "\n",
    "Final_prediction= pd.concat([Id_test,test_prediction],axis=1)\n",
    "Final_prediction.to_csv(\"test_score.csv\", index=False)\n",
    "Final_result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
